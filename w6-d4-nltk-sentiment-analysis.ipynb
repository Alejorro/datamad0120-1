{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@b.terryjack/nlp-pre-trained-sentiment-analysis-1eb52a9d742c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Letâ€™s evaluate some pretrained **sentiment analysis tools provided in various Pythonic Natural Language Processing libraries.**\n",
    "- NLTK (Natural Language Toolkit):\n",
    "\n",
    "- TextBlob\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.nltk.org/api/nltk.sentiment.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install nltk\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A SentimentAnalyzer is a tool to implement and facilitate Sentiment Analysis tasks using NLTK features and classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#English/spanish \n",
    "#compound [-1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.413, 'neu': 0.587, 'pos': 0.0, 'compound': -0.2755}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence=\"I do not like you\"\n",
    "sia.polarity_scores(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.192, 'pos': 0.808, 'compound': 0.6369}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence='i love you'\n",
    "sia.polarity_scores(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.527, 'neu': 0.473, 'pos': 0.0, 'compound': -0.6652}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence='but you are dumb sometimes'\n",
    "sia.polarity_scores(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.667, 'pos': 0.333, 'compound': 0.4588}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence=\"it's fucking raining in my heart :)\"\n",
    "sia.polarity_scores(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://textblob.readthedocs.io/en/dev/quickstart.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install textblob\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextBlob aims to provide access to common text-processing operations through a familiar interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#English\n",
    "#polarity[-1,1]\n",
    "#subjectivity[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.85, subjectivity=1.0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(\"Hello beautiful\").sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='great project'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.8, subjectivity=0.75)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob = TextBlob(text)\n",
    "blob.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have a good day\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.7, subjectivity=0.6000000000000001)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#translate\n",
    "en_blob = TextBlob(u'que tengas buen buen day')\n",
    "en_blob=en_blob.translate(from_lang='es',to='en')\n",
    "print(en_blob)\n",
    "en_blob.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"I have good spelling!\")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = TextBlob(\"I havv goood speling!\")\n",
    "b.correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['cat', 'dog', 'octopus'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "animals = TextBlob(\"cat dog octopus\")\n",
    "animals.words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word by word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model form \n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Last', 'weekend', 'was', 'horrible', 'because', 'we', 'did', \"n't\", 'have', 'a', 'project', ',', 'but', 'how', 'great', 'is', 'this', 'weekend', 'going', 'to', 'be', '?', '?']\n"
     ]
    }
   ],
   "source": [
    "text=\"Last weekend was horrible because we didn't have a project, but how great is this weekend going to be??\"\n",
    "words = nltk.word_tokenize(text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Last',\n",
       " 'weekend',\n",
       " 'was',\n",
       " 'horrible',\n",
       " 'because',\n",
       " 'we',\n",
       " 'didn',\n",
       " 't',\n",
       " 'have',\n",
       " 'a',\n",
       " 'project',\n",
       " 'but',\n",
       " 'how',\n",
       " 'great',\n",
       " 'is',\n",
       " 'this',\n",
       " 'weekend',\n",
       " 'going',\n",
       " 'to',\n",
       " 'be']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removing symbols\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokens = tokenizer.tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last : Sentiment(polarity=0.0, subjectivity=0.06666666666666667)\n",
      "weekend : Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "was : Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "horrible : Sentiment(polarity=-1.0, subjectivity=1.0)\n",
      "because : Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "we : Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "did : Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "n't : Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "have : Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "a : Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "project : Sentiment(polarity=0.0, subjectivity=0.0)\n",
      ", : Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "but : Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "how : Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "great : Sentiment(polarity=0.8, subjectivity=0.75)\n",
      "is : Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "this : Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "weekend : Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "going : Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "to : Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "be : Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "? : Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "? : Sentiment(polarity=0.0, subjectivity=0.0)\n"
     ]
    }
   ],
   "source": [
    "for w in words:\n",
    "    print(w,':' ,TextBlob(w).sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.5, subjectivity=1.0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob('HELLOOO :) ??').sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'under', 'ma', 'shan', 'whom', \"that'll\", 'those', 'my', 'when', 'up', 've', 'mustn', 'all', 'is', 'will', 'him', 'because', 'few', 'in', 'only', \"shouldn't\", 'has', \"doesn't\", 'their', 'do', 'below', \"weren't\", 'now', 'they', \"aren't\", 'aren', \"hasn't\", 'both', 'ourselves', 'she', \"you'd\", 'yourself', 'with', 'doing', 'y', 'he', 'on', \"isn't\", 'once', 'weren', 'your', 'same', 'yours', 'more', 'why', 'have', \"she's\", 'where', 'here', 'm', 'd', \"didn't\", 'being', 'had', 'for', 'to', 'can', 'the', \"mustn't\", 'needn', 'a', \"you'll\", \"you've\", 'very', \"don't\", \"it's\", 'or', 'no', \"wouldn't\", 'shouldn', 'and', 'while', 'been', 'most', 'nor', 'isn', \"needn't\", 'are', 'just', 'an', 'hers', 'having', 'after', 'yourselves', 'from', 'down', \"hadn't\", 're', 'were', 'so', 'above', 'we', 'over', 'hasn', 'his', \"haven't\", 'at', 'any', 'some', 'does', 'which', 'did', 'into', 'was', 'herself', \"should've\", 'll', 'through', 'what', 'each', 'again', 'how', 'themselves', 'doesn', 'too', 'these', 'about', 'didn', \"wasn't\", 'our', 'myself', 'until', 'be', 'o', 'itself', 'mightn', 's', 'between', 'won', 'couldn', 'if', 'off', 'them', 'such', 'this', 'as', 'haven', 'that', 'other', 'further', 'me', 'than', 'you', 'i', 'ours', \"mightn't\", 'during', \"couldn't\", 'by', 'of', 'then', 'don', \"you're\", 'her', 'am', 'there', 'out', 'who', 'but', 'wasn', \"shan't\", 'before', 'not', 't', 'wouldn', 'hadn', 'himself', \"won't\", 'own', 'against', 'theirs', 'it', 'its', 'should', 'ain'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/blancalluch/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Last', 'weekend', 'horrible', 'project', 'great', 'weekend', 'going']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stopwords --> las palabras mÃ¡s comunes que no tienen ninguna connotacion y\n",
    "#puedes eliminar para analizar el texto sin ellas.\n",
    "#En el caso de querer analizar todo el texto entero serÃ­a mejor no eliminarlas\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)\n",
    "\n",
    "tokens_clean = [e for e in tokens if e not in stop_words]\n",
    "tokens_clean"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
