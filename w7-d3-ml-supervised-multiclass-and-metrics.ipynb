{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised learning mutliclass & metrics\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/700/1*x7P7gqjo8k2_bj2rTQWAfg.jpeg\" width=\"400\" />\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1000/1*erhS3Y1ZtN3bcJAYgaLC_g.gif\" width=\"300\" />\n",
    "<img src=\"https://img.devrant.com/devrant/rant/r_1587555_kpz6c.jpg\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/fit/t/1600/480/1*Ub0nZTXYT8MxLzrz0P7jPA.png\" width=\"600\"/>\n",
    "\n",
    "* **Accuracy:** `Accuracy = N/NGT`\n",
    "  - `N` number of correctly classified samples\n",
    "  - `NGT` Total number of samples to be classified\n",
    "  - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score\n",
    "* **Precision:** `tp / (tp + fp)`\n",
    "  - The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.\n",
    "  - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score\n",
    "* **Recall** `tp / (tp + fn)`\n",
    "  - The recall is intuitively the ability of the classifier to find all the positive samples.\n",
    "  - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score\n",
    "  \n",
    "* **F1Score**: `F1 = 2 * (precision * recall) / (precision + recall)`\n",
    "\n",
    "  - The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0\n",
    "  - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "```\n",
    "\n",
    "<img src=\"https://upl7s2nmwf2w5a6vghztmvqp-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/Figure-2-ivu.jpg\" width=\"400\"/>\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/4420/1*btcfBuM5Eqqc6rJ3iw3sNQ.png\" width=\"400\"/>\n",
    "\n",
    "\n",
    "\n",
    "### Difference between model evaluation metrics\n",
    "* **Accuracy vs Precision** -> https://en.wikipedia.org/wiki/Accuracy_and_precision\n",
    "* **Precision & Recall**\n",
    "  - https://towardsdatascience.com/beyond-accuracy-precision-and-recall-3da06bea9f6c\n",
    "  - https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html\n",
    "  \n",
    "### Refs\n",
    "- https://scikit-learn.org/stable/modules/model_evaluation.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## ROC Curve and AUC\n",
    "Receiver Operator Characteristic Curve from `sklearn.metrics`\n",
    "- http://arogozhnikov.github.io/2015/10/05/roc-curve.html\n",
    "- http://www.navan.name/roc/\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import roc_auc_score\n",
    "y_true = np.array([0, 0, 1, 1])\n",
    "y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
    "\n",
    "# Extract AUC score\n",
    "roc_auc_score(y_true, y_scores)\n",
    "\n",
    "# Extract ROC curve\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y, y_scores, pos_label=1)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep dive on ROC curve\n",
    "\n",
    "**In Sklearn** estimators have a score method providing a default evaluation criterion for the problem they are designed to solve\n",
    "- In the case of `Logistic Regresion` is the mean accuracy on the given test data and labels.\n",
    "- In probabilistic classifers `classifier.predict()` **decision threshold is 0.5**\n",
    "\n",
    "\n",
    "- http://arogozhnikov.github.io/2015/10/05/roc-curve.html\n",
    "- http://www.navan.name/roc/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation\n",
    "\n",
    "`from sklearn.model_selection import cross_val_score`\n",
    "\n",
    "<img src=\"https://scikit-learn.org/stable/_images/grid_search_cross_validation.png\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep dive on ROC curve\n",
    "\n",
    "\n",
    "- **ROC**: https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html#sphx-glr-auto-examples-model-selection-plot-roc-py\n",
    "\n",
    "- **ROC with Cross Validation**: https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc_crossval.html#sphx-glr-auto-examples-model-selection-plot-roc-crossval-py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract a basic report\n",
    "\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "# Precision, Recall, F1-Score & Support\n",
    "print(classification_report(y_test,y_pred))\n",
    "# Accuracy Score -> in mutliclass is computed with jaccard_score\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass models training\n",
    "\n",
    "While some models can handle mutliclass data inherently, some others require a strategy to **convert** the mutliclass problem into **multiple binary classification problems**.\n",
    "- Multiclass as **One-Vs-One**\n",
    "- Multiclass as **One-Vs-The-Rest**\n",
    "\n",
    "- https://scikit-learn.org/stable/modules/multiclass.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass metrics \n",
    "\n",
    "* **Balanced accuracy score:** The balanced accuracy in binary and multiclass classification problems to deal with imbalanced datasets. It is defined as the average of recall obtained on each class.\n",
    "    - `sklearn.metrics.balanced_accuracy_score`\n",
    "* **Weighted ROC Curve for multiclass & multilabel:** `sklearn.metrics.roc_auc_score`\n",
    "    - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score\n",
    "\n",
    "* **Confussion matrix:** Each row of the matrix represents the instances in a predicted class while each column represents the instances in an actual class (or vice versa).\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test,y_pred)\n",
    "```\n",
    "\n",
    "**Note:** Confussion Matrix can be ploted with seaborn heatmap\n",
    "- https://seaborn.pydata.org/generated/seaborn.heatmap.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
