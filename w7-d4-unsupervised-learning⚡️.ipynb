{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Unsupervised learning - Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://lh3.googleusercontent.com/DS4BHTkXT_9FzxuOd67PNjJT-o87kdtvP42wq_JUzQz8oWhzOOxWKu0CAkTSzBzyLKrYNWAF8dAY6FUSgjLJFBBrMjHz_cdk9-i0QhAOnIdo8Nq3192BdGxlEUwRRpCzkp_iBiIK\" width=\"400\"/>\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/694/1*5RDVF1xW0LfXjoxZp6jI1Q.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kmeans clustering\n",
    "\n",
    "**Kmeans** is one of the most popular **clustering** algorithms. K-means stores k centroids that it uses to define clusters. A point is considered to be in a particular cluster if it is closer to that cluster‚Äôs centroid than any other centroid. K-Means finds the best centroids by alternating between (1) assigning data points to clusters based on the current centroids (2) choosing centroids (points which are the center of a cluster) based on the current assignment of data points to clusters.\n",
    "\n",
    "<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/merge3cluster.jpg\" width=\"300\"/>\n",
    "\n",
    "**The Kmeans algorithm**\n",
    "1.  Select `K` random starting cluster centroids\n",
    "2.  Compute the distance between each observation and the clusters\n",
    "3.  Reassign a cluster to each observation and then recompute the centroids\n",
    "4.  Keep doing so until the labels stay constant and we no longer need to reassign\n",
    "\n",
    "\n",
    "Voronoi diagram: \n",
    "- http://paperjs.org/examples/voronoi/\n",
    "- http://www.raymondhill.net/voronoi/rhill-voronoi.html\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Other Clustering algorithms\n",
    "\n",
    "<img src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_cluster_comparison_0011.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Bakery dataset  in sklearn\n",
    "\n",
    "- https://github.com/boyander/datamad-1019/blob/master/w7-d4-unsupervised-pipeline/w7-d4-unsupervised-learning.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised learnign cluster metrics\n",
    "- **Sillouette score**: How dense the clusters are they and how well separated. CContrasts the average distance to elements in the same cluster with the average distance to elements in other clusters.\n",
    "  - This score favors convex clusters, for many non convex datasets it will give an artificially low score\n",
    "  - mean ratio of intra-cluster and nearest-cluster distance\n",
    "  \n",
    "<img src=\"https://image3.slideserve.com/6607494/limitations-of-k-means-non-convex-shapes-l.jpg\" width=\"300\"/>\n",
    "\n",
    "**Note:** A convex polygon is a simple polygon (not self-intersecting) in which no line segment between two points on the boundary ever goes outside the polygon. Equivalently, it is a simple polygon whose interior is a convex set.[1] In a convex polygon, all interior angles are less than or equal to 180 degrees, while in a strictly convex polygon all interior angles are strictly less than 180 degrees.\n",
    "  \n",
    "- **Distortion**: Sum of squared distances of samples to their closest cluster center. `KMeans(3).fit(iris.data).intertia_`\n",
    "\n",
    "**Note:** Smaller `distortion` means more dense clusters.\n",
    "\n",
    "- **Calinski Harabaz**: The score is defined as ratio between the within-cluster dispersion and the between-cluster dispersion. `sklearn.metrics.calinski_harabasz_score`\n",
    "\n",
    "\n",
    "- **T-SNE Plots**: Cluster visualization. Like PCA, embbed N dimensions into 2D space.\n",
    "    - https://scikit-learn.org/stable/auto_examples/manifold/plot_manifold_sphere.html#sphx-glr-auto-examples-manifold-plot-manifold-sphere-py\n",
    "    - https://www.youtube.com/watch?v=NEaUSP4YerM\n",
    "    \n",
    "<img src=\"https://i.stack.imgur.com/OxEW5.png\" width=\"400\"/>\n",
    "\n",
    "\n",
    "\n",
    "- **Elbow method**: Helps select the optimal number of clusters by fitting the model with a range of values for ùêæ\n",
    "\n",
    "```python\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "# Generate synthetic dataset with 8 random clusters\n",
    "X, y = make_blobs(n_samples=1000, n_features=12, centers=8, random_state=42)\n",
    "\n",
    "# Instantiate the clustering model and visualizer\n",
    "model = KMeans()\n",
    "visualizer = KElbowVisualizer(\n",
    "    model, k=(4,12), metric='calinski_harabasz', timings=False\n",
    ")\n",
    "\n",
    "visualizer.fit(X)        # Fit the data to the visualizer\n",
    "visualizer.show()        # Finalize and render the figure\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Bakery dataset  in Apache Spark\n",
    "\n",
    "- https://github.com/boyander/datamad-1019/blob/master/w7-d5-spark-intro/spark-intro.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Generative Models - Unsupervised learning\n",
    "\n",
    "- **Generative GMM:** https://jakevdp.github.io/PythonDataScienceHandbook/05.12-gaussian-mixtures.html\n",
    "\n",
    "Articles:\n",
    "- https://openai.com/blog/generative-models/\n",
    "- https://towardsdatascience.com/generative-deep-learning-lets-seek-how-ai-extending-not-replacing-creative-\n",
    "process-fded15b0561b\n",
    "- https://www.youtube.com/watch?v=G5JT16flZwM&feature=emb_logo\n",
    "- https://pathmind.com/wiki/generative-adversarial-network-gan\n",
    "\n",
    "Articles on autoencoders:\n",
    "- https://medium.com/intuitive-deep-learning/autoencoders-neural-networks-for-unsupervised-learning-83af5f092f0b\n",
    "- https://towardsdatascience.com/pca-vs-autoencoders-1ba08362f450"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
